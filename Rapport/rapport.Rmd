---
title: "Projet SY19 - Régression et classification"
author: "Tom Bourg, Guillaume Sabbagh, Jiali Xu"
date: "06/01/2021"
lang: "fr-fr"
geometry: margin=2cm
output:
  bookdown::pdf_document2 : 
    toc : false
    extra_dependencies: ["float"]
    
---

```{r, include=FALSE}
knitr::opts_chunk$set(fig.align = "center")
knitr::opts_chunk$set(fig.pos = "H", out.extra = "")
#load("env.RData")
```

# Phoneme

```{r, include=FALSE}
load("phoneme.RData")
library(keras)
model.serialize <- serialize_model(load_model_hdf5("keras_out.h5"))
model <- unserialize_model(model.serialize)
```

## Analyse exploratoire

Dans cette première partie, il s'agit d'une classification des phonemes. Le jeu de données contient 2250 observations avec 256 préditeurs et une seule variables de réponse. On observe que la classification est en 5 classes et tous les préditeurs sont quantitatives. 

```{r phoneme-aexp, echo=FALSE}
summary(data$y)
```

On remarque ici que le nombre d'observation n'est pas balancé parmi les classes. 

## Analyse de Composantes Principales 

On s'intéresse à une analyse de composantes principales afin de réduire le nombre de préditeurs sachant que 256 est assez nombreux. A l'aide de la fonction *prcomp()*, on trace la proportion de variance expliquée cumulée en fonction du nombre de composante dans la figure \@ref(fig:phoneme-pca) dessous. 

```{r phoneme-pca, echo=FALSE, fig.cap="Proportion de variance expliquée cumulée", out.width="50%"}
plot(cumsum(res.pca$sdev^2 / sum(res.pca$sdev^2)), type="l", ylim=0:1, ylab = "Cumulative Proportion", xlab = "number of PCs")
```

On constate que les 107 premières composantes expliquent 95% de variance. Du fait que pour les réseaux neurones, le nombre de ports d'entrée de chaque couche est préférable d'être exponentiel de 2, on choisit ici 128 composantes pour les traitements suivants.

## Apprentissage de modèles

### LDA, QDA et Naive-Bayes

Le package **MASS** nous propose des fonctions pour LDA et QDA et le pakcage **naivebayes** pour un modèle Naive-Bayes. On observe que le taux d'erreur du modèle QDA est beaucoup plus haut que les deux autres. De ce fait, on constate que la matrice de covariance pour chaque classe reste presque identique. 

### Regression Logistique

Afin de réaliser une regression logistique pour une classification multinomiale, on utilise le modèle *multinom()* dans le package **nnet**. A partir de cette méthode, on réalise également un modèle additive généralisé en ajoutant un *natural cubic spline*. 

Ensuite, on teste la regression ridge et lasso. Le package **glmnet** nous permet de trouver *lambda* optimisé en faisant une validation croisée. Le modèle lasso nous donne un meilleur résultat. 

### Arbres de regression

Pour les méthodes basées sur arbre, on a essayé une arbre binaire de décision *rpart()* du package **rpart** et son élagage en choisissant une valeur **CP** qui minimise l'erreur de validation croisée. On a également réalisé un modèle **Bagging** et **RandomForest** en appliquant différentes valeurs de *mtry*.

### SVM

La fonction *ksvm* du package **kernlab** est utilisée pour tester avec deux noyaux différents. Pour chaque noyau, on fait d'abord une validation croissée pour trouve une valeur **C** optimisée parmi un ensemble [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10e4].

```{r phoneme-svm, echo=FALSE, fig.cap="Performance de modèles", fig.show="hold", out.width="45%"}
CC<-c(0.001,0.01,0.1,1,10,100,1000,10e4)
plot(CC,svm.linear.c,type="b",log="x",xlab="C",ylab="CV error (linear)")
plot(CC,svm.gaus.c,type="b",log="x",xlab="C",ylab="CV error (gaussian)")
```

### Réseaux neurones

#### Réseau neurone artificiel

On s'intéresse d'abord au réseau neurone artificiel avec deux couches. Sachant que nous sommes sur un problematique de classification, la fonction d'activation pour la couche de sortie est donc *softmax*. On paramètre *batch_size* à 128 et *epoch* à 30. 

#### Réseau neurone profond régularisé

Un réseau neurone profond est un réseau neurone artificiel avec plusieurs couches. Ici, on réalise un réseau neurone profond avec 4 *layer_dense* et 2 *layer_dropout*. On fixe le coefficient à 0.5 pour les couche de régulation afin d'éviter un surapprentissage. On applique une fonction d'activation *relu* pour tous les couches sauf celui de sortie. Les hyperparamètres à optimiser sont donc *units* des deux couches au milieu. Après des essais en utilisant une validation croisée, on n'observe pas une amélioration très évidente pour les différentes valeurs.

#### Réseau neurone convolutionnel

Enfin, on effectue un essai sur un réseau neurone convolutionnel. Pour ce faire, il nous faut d'abord convertir les données en matrice 3D. Dans ce cas, la dimension de matrice de notre données est 2250\*128 (après extraction de caractéristique). On choisit de la convertir en dimension 2250\*16\*8. La structure de modèle est comme ci-dessous. 

```{r phoneme-rnc, echo=FALSE}
summary(model)
```

## Comparaison de modèle

A la fin, on fait une comparaison de tous les modèles réalisés. Voir les performance sur figure \@ref(fig:phoneme-mdl) et \@ref(fig:phoneme-nn)

```{r phoneme-mdl, echo=FALSE, fig.cap="Performance de modèles", out.width="120%"}
boxplot(lda.err, qda.err, nb.err,
        multinom.err, ridge.err, lasso.err,
        gam.err,
        rpart.err, prune.err, 
        bagged.err, rf.err,
        svm.linear.err, svm.gaus.err,
        names=c("lda", "qda", "nb", 
                "lr", "rid", "las",
                "gam",
                "rpa", "pru",
                "bag", "rf",
                "svml", "svmg"))
```

```{r phoneme-nn, echo=FALSE, fig.cap="Performance de réseau neurone", out.width="50%"}
boxplot(ann.err,
        dnn.err,
        cnn.err,
        names=c("ann", 
                "dnn",
                "cnn"))
```

On constate que le modèle réseau neurone profond nous donne une précision la plus haute et une variance plus petite. 

# Bike rental dataset

## Travail préliminaire

Le jeu de donnée contient 12 variables explicatives pour une variable à prédire. On cherche à prédire le nombre de vélo loué une journée en fonction de la saison, des conditions météorologiques, des jours ouvrés etc. Il s'agit d'une tâche de régression. Nous disposons pour cela de 365 observations ce qui est relativement peu. Pour un petit dataset comme celui-là, nous avons décidé d'éliminer d'office les réseaux neuronaux qui nécessitent de grands jeux de données pour converger. Nous essaieront donc les principaux algorithmes d'apprentissage statistique nécessitant peu de données pour fonctionner.

De plus, 7 des 12 prédicteurs sont des variables qualitatives ! Il nous faudra donc des méthodes capables de gérer des variables qualitatives en même temps que des variables quantitative. On pense directement aux arbres de décision qui sont plutôt versatiles. On pouvait penser que certaines variables serait corrélées (saison et mois par exemple) mais puisque la plupart des variables sont qualitatives, une ACP ne fonctionnerait pas. On va donc compter sur les modèles pour sélectionner les bonnes variables. Encore une fois les arbres de décision devraient bien s'en sortir.

## KNN regression

En premier modèle, nous avons testé le modèle KNN, sans nous attendre à ce qu'il performe bien puisque ce dernier ne peut gérer les variables qualitatives. Le paramètre k a été trouvé par nested cross-validation. Comme prévu, le modèle ne performe pas particulièrement et notre meilleure erreur quadratique moyenne est de 483209.3 pour k=7.

![Modèle KNN : Erreur quadratique moyenne en fonction de k, le minimum est atteint en k=7.](./bike_knn.png){#id .class width=12cm height=6cm}

![Boxplot de l'erreur quadratique moyenne pour le modèle de régression KNN.](./bike_knn_error.png){#id .class width=12cm height=6cm}

## Elastic-net regression

Notre prochain modèle va permettre de faire de la sélection de variable. Le modèle Elastic-net, et ses cas particuliers ridge et lasso, permettent de faire une régression en sélectionnant les variables les plus utiles à la prédicitions. Cependant, ils ne se comportent pas bien avec les données qualitatives. Et malheureusement cela rend les modèles peu précis tout comme le modèle KNN (cf. les boxplots d'erreurs).


![Boxplot des erreurs quadratiques moyennes pour les méthodes lasson ridge ainsi que elastic net.](./elastic_net.png){#id .class width=12cm height=6cm}


## SVM

Un SVM pourrait correctement traiter nos données avec un kernel adéquat sachant que nos données ne sont pas linéaires.

L'astuce est de trouver le bon kernel, ce qui n'est pas simple.

Avec un kernel radial, on trouve par nested cross-validation que le paramètre gamma doit être 1/7. On obtient alors une erreur quadratique moyenne d'environ 250000.

Avec un kernel polynomial, on trouve par nested cross-validation les paramètres du kernel polynomial et on obtient finalement une erreur quadratique moyenne d'environ 800000.

Le kernel linéaire donne de très mauvais résultats comme prévu (mse supérieure à 500000).

![Boxplots des erreurs quadratiques moyennes pour les SVM avec kernel radial et polynomial.](./svm.png){#id .class width=12cm height=6cm}


## Decision tree et random forest

C'est la méthode qui semble la plus prometteuse depuis l'analyse préliminaire. En effet, elle est robuste aux données qualitatives et permet aussi de sélectionner les prédicteurs les plus discriminants.

L'arbre de décision seul a tendance à overfit, on preférera donc les random forest qui généralisent mieux.

Comme prévu, l'arbre de décision seul overfit et donne de mauvais résultats : l'erreur quadratique moyenne est d'environ 450000.

La forêt quant à elle n'overfit pas et donne de bons résultats, l'erreur quadratique moyenne est d'environ 250000. Elle égalise donc les performances du svm avec le kernel radial.

![Boxplots des erreurs quadratiques moyennes pour les arbres et forêts de régression.](./tree.png){#id .class width=12cm height=6cm}

## Sélection de modèle

Voilà un récapitulatif général des mse obtenues :

![Boxplots des erreurs quadratiques moyennes pour toutes nos méthodes.](./recap.png){#id .class width=18cm height=9cm}


Nous avons donc décidé de retenir la SVM avec kernel radial.
